{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6505dfe5",
   "metadata": {},
   "source": [
    "# Changelog\n",
    "This is a written documentation of big bugs I come across, what I did to solve them, and comments moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77997d83",
   "metadata": {},
   "source": [
    "### Fixes for Dimensionality Reduction in Large Datasets\n",
    "Date: 2025 -09\n",
    "\n",
    "#### Summary:\n",
    "Stabilized dimensionality reduction pipeline for both small (100-node) and large (700-node) datasets. Eliminated degenerate “all zeros” reductions and fixed optimizer stalls in higher-order correlation levels.\n",
    "\n",
    "\n",
    "timecorr/helpers.py || weighted_timepoint_decoder -> optimize_weights\n",
    "* Passed in a opt_init value of random, this gives a random starting point, to help with getting stuck on the first iteration and not progressing further. (as gradient descent too flat)\n",
    "* Inside of minimize fucntion, added method='SLSQP'. This matches the original implementation is scipy 1.2.1. It has obvously been awhile since then and this is necessary as the minimize function has been updated in the recent years.\n",
    "\n",
    "\n",
    "Changes\n",
    "timecorr/braintools/brain_reduce.py || reduce\n",
    "* replaced \"Rows <= ndims: returning zeros.\" behavior with a clamp:\n",
    "if n_comp is not None and rows <= n_comp:\n",
    "    model_params['n_components'] = max(1, rows - 1)\n",
    "Ensures reductions always return meaningful features, even when rows < ndims.\n",
    "timecorr/helpers.py → reduce\n",
    "\n",
    "* padding/truncation now handles list of arrays and ndarrays separately.\n",
    "Prevents ValueError: all input arrays must have same number of dimensions during np.hstack.\n",
    "Guarantees reduced outputs are always shaped (rows, V) per timepoint.\n",
    "timecorr/helpers.py → weighted_timepoint_decoder\n",
    "\n",
    "* Impact:\n",
    "Higher-order correlation decoding now stable for both 100-node and 700-node dataset, and optimization no longer collapses to uniform weights due to zeroed features.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
