{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6505dfe5",
   "metadata": {},
   "source": [
    "# Changelog\n",
    "This is a written documentation of big bugs I come across, what I did to solve them, and comments moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77997d83",
   "metadata": {},
   "source": [
    "### Fixes for Dimensionality Reduction in Large Datasets\n",
    "Date: 2025 -09\n",
    "\n",
    "#### Summary:\n",
    "Stabilized dimensionality reduction pipeline for both small (100-node) and large (700-node) datasets. Eliminated degenerate “all zeros” reductions and fixed optimizer stalls in higher-order correlation levels.\n",
    "\n",
    "\n",
    "timecorr/helpers.py || weighted_timepoint_decoder -> optimize_weights\n",
    "* Passed in a opt_init value of random, this gives a random starting point, to help with getting stuck on the first iteration and not progressing further. (as gradient descent too flat)\n",
    "* Inside of minimize fucntion, added method='SLSQP'. This matches the original implementation is scipy 1.2.1. It has obvously been awhile since then and this is necessary as the minimize function has been updated in the recent years.\n",
    "\n",
    "\n",
    "Changes\n",
    "timecorr/braintools/brain_reduce.py || reduce\n",
    "* replaced \"Rows <= ndims: returning zeros.\" behavior with a clamp:\n",
    "if n_comp is not None and rows <= n_comp:\n",
    "    model_params['n_components'] = max(1, rows - 1)\n",
    "Ensures reductions always return meaningful features, even when rows < ndims.\n",
    "timecorr/helpers.py → reduce\n",
    "\n",
    "* padding/truncation now handles list of arrays and ndarrays separately.\n",
    "Prevents ValueError: all input arrays must have same number of dimensions during np.hstack.\n",
    "Guarantees reduced outputs are always shaped (rows, V) per timepoint.\n",
    "timecorr/helpers.py → weighted_timepoint_decoder\n",
    "\n",
    "* Impact:\n",
    "Higher-order correlation decoding now stable for both 100-node and 700-node dataset, and optimization no longer collapses to uniform weights due to zeroed features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930a113",
   "metadata": {},
   "source": [
    "### Fixes for decresing accuracy as levels went up\n",
    "Date: 2025 -11\n",
    "\n",
    "#### Summary:\n",
    "Stabilized gradient descent so that you will not get stuck on the wrong gradient descent path. Previously, the accuracys as you increase higher order correlation would sometimes decrease. But this should not be the case because we should find the optimal weights even if that is the same weights as the level before.\n",
    "\n",
    "\n",
    "timecorr/helpers.py || weighted_timepoint_decoder\n",
    "Changes:\n",
    "Track best_acc, best_mu_vec, best_row across optimize_levels.\n",
    "Accept if accuracy ≥ best; otherwise record prior best and zero newest weight.\n",
    "Add _mu_as_vector() to normalize optimizer output (fixes KeyError: np.int64(0)).\n",
    "Add _pad_mu() to align vector lengths (fixes IndexError on rejections).\n",
    "\n",
    "Behavior:\n",
    "Accuracy per fold no longer decreases with level; mean across folds also non-decreasing.\n",
    "When higher levels don’t help, their weights become 0, effectively reverting to the best lower level.\n",
    "\n",
    "Impact:\n",
    "More stable curves; same schema (weights level_0..level_k), with some newest weights recorded as 0.0.\n",
    "\n",
    "\n",
    "\n",
    "* Impact:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
